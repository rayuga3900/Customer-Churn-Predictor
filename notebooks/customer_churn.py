# -*- coding: utf-8 -*-
"""customer_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tA1JBevEr6iJ-Dd7kQXK_KR16EJoumPZ
"""

import numpy as np
import pandas as pd
import kagglehub
import seaborn as sns
import matplotlib.pyplot as plt
from kagglehub import KaggleDatasetAdapter
from sklearn.preprocessing import OneHotEncoder
file_path = "WA_Fn-UseC_-Telco-Customer-Churn.csv"

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "blastchar/telco-customer-churn",
  file_path,

)
print(df.describe())
print(df.info())
#print("First 5 records:", df.head())

"""Some categories are stored as number type so changing their datatype"""

df['SeniorCitizen'] = df['SeniorCitizen'].astype('object')
df.info()

"""Some objects are numeric values converting their types"""

df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan) #changing the blank space[missing value] as nan

df['TotalCharges'] = df['TotalCharges'].astype(float)
df.info()

""" Dropping the IDs because they don't provide useful information for the model to learn patterns"""

x = df.drop(['Churn','customerID'],axis = 1)
y = df['Churn']

x

"""The reason we do train-test split before separating numeric and categorical columns is:

If we split after preprocessing, the model may learn from the entire dataset, including what should be unseen test data. This causes data leakage and gives an unfairly good result.

Splitting first ensures the test data remains completely new for the model, making the evaluation honest and reliable.
"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)

x_train

x_num = x_train.select_dtypes(include = ['int64','float64'])
x_cat = x_train.select_dtypes(include = ['object'])
# df.select_dtypes(include=['number']).describe()

x_num

x_cat

"""Checking for the outliers"""

outliers_dict = {}

for col in x_num.columns:
    q1 = x_num[col].quantile(0.25)
    q3 = x_num[col].quantile(0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    mask = (x_num[col] < lower) | (x_num[col] > upper)
    outliers_dict[col] = x_num[col][mask]

# Print outliers per column
for col, outliers in outliers_dict.items():
    print(f"Outliers in {col}:\n{outliers}\n")

"""Checking for the skewness
| **Skewness Value** | **Interpretation**           |
| ------------------ | ---------------------------- |
| -0.5 to 0.5        | Approximately symmetric      |
| 0.5 to 1           | Moderately positively skewed |
| Greater than 1     | Highly positively skewed     |
| -1 to -0.5         | Moderately negatively skewed |
| Less than -1       | Highly negatively skewed     |

"""

x_num.skew()#no skewness

"""Checking the missing values in categories and numeric"""

print(x_cat.isna().sum())
print(x_num.isna().sum())

"""checking the correlation

[|corr| ≥ 0.8 → consider dropping (for linear models)]
"""

x_num.corr()

"""Dropping the highly correlated features from train and tedst"""

x_num = x_num.drop('TotalCharges',axis=1)
x_test = x_test.drop('TotalCharges',axis=1)

print(x_num.info())
print(x_test.info())

"""Encoding the categorical columns

Feature type              | Model type    | Recommended encoder
------------------------- | ------------- | ---------------------------------
Nominal, low-cardinality  | Any           | OneHotEncoder
Ordinal                   | Any           | OrdinalEncoder
Nominal, high-cardinality | Tree-based    | OrdinalEncoder / FrequencyEncoder
Nominal, high-cardinality | Linear models | TargetEncoder / BinaryEncoder

"""

ohe  = OneHotEncoder(drop='first', handle_unknown='ignore')
x_cat_encoded = ohe.fit_transform(x_cat).toarray()
x_cat_encoded = pd.DataFrame(x_cat_encoded, columns=ohe.get_feature_names_out(x_cat.columns), index=x_cat.index)
x_test_num = x_test.select_dtypes(include=['int64','float64'])
x_cat_test = x_test.select_dtypes(include=['object'])
x_cat_test_encoded = ohe.transform(x_cat_test)

x_cat_test_encoded = pd.DataFrame(
    x_cat_test_encoded.toarray(),
    columns=ohe.get_feature_names_out(x_cat.columns),
    index=x_cat_test.index
)
x_cat_encoded.head()
# x_cat_test_encoded.head()

"""Checking for class imbalance
| **Minority:Majority Ratio** | **Interpretation** | **Suggested Action**               |
| --------------------------- | ------------------ | ---------------------------------- |
| 1 – 1.5                     | Balanced           | No action needed                   |
| 1.5 – 3                     | Slight imbalance   | Consider `class_weight`            |
| 3 – 5                       | Moderate imbalance | Use `class_weight` or oversampling |
| Greater than 5              | Severe imbalance   | SMOTE / undersampling required     |

"""

count = y_train.value_counts()

ratio = count.max() / count.min()

print(f"Ratio: {ratio:.2f}x")
print("Imbalanced" if ratio > 1.5 else "Balanced")
print(count)

x_cat['Contract'].value_counts()

x_cat.isna().sum()

x_num.isna().sum()

"""Starting the feature selection

1. Using VarianceThreshold[It is used only for features for checking their variance ]
"""

from sklearn.feature_selection import SelectKBest, chi2, f_classif , VarianceThreshold, mutual_info_classif
var_thres = VarianceThreshold(threshold=0.1)
var_thres.fit(x_num)
var_thres.get_support()

# checking if any numeric column is not varying and needs to be droppedk

drop_cols = x_num.columns[~var_thres.get_support()]
drop_cols

"""2.Using Correlation[Only for features not for feature vs target because target is categorical.so correlation might be misleading.It is mostly used for numeric cols only]

| Correlation Value | Interpretation          | Action                              |
| ----------------- | ----------------------- | ----------------------------------- |
| 0 – 0.3           | Weak correlation        | Usually keep both features          |
| 0.3 – 0.7         | Moderate correlation    | Check relevance                     |
| 0.7 – 0.8         | Strong correlation      | Consider dropping one               |
| > 0.8             | Very strong correlation | Drop one to avoid multicollinearity |

"""

corr = x_num.corr()
corr

#checking correlation among features and dropping highly correlated features if any
for i in range(len(corr.columns)):
  for j in range(i):
    if(abs(corr.iloc[i,j]) > 0.8):
      col = corr.columns[i]
      print(col)

"""3. Using Mutual Information mutual_info_classif

MI is univariate means it looks at individual relation with the target rather than looking at the combination .
Mutual Information is a univariate feature selection method, meaning it measures the relationship between each individual feature and the target variable independently.

| **Mutual Information (MI) Value** | **Interpretation**          | **Feature Usefulness** |
| --------------------------------- | --------------------------- | ---------------------- |
| **0**                             | No relationship with target | No information         |
| **0.001 – 0.005**                 | Extremely weak relationship | Almost useless         |
| **0.006 – 0.02**                  | Weak relationship           | Low importance         |
| **0.03 – 0.04**                   | Moderate relationship       | Useful                 |
| **0.05 – 0.07**                   | Strong relationship         | Very useful            |
| **≥ 0.08**                        | Very strong relationship    | Highly predictive      |
"""

X_encoded = pd.concat([x_num, x_cat_encoded], axis=1)
X_test_encoded = pd.concat([x_test_num, x_cat_test_encoded], axis=1)
mi = mutual_info_classif(X_encoded, y_train)
mi_scores = pd.Series(mi, index=X_encoded.columns)
mi_scores.sort_values(ascending=False)

drop_columns = mi_scores[mi_scores.values==0.00]

drop_columns

X_encoded = X_encoded.drop(drop_columns.index, axis=1)
X_test_encoded = X_test_encoded.drop(drop_columns.index, axis=1)

X_encoded.head()

"""Training the model"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

import seaborn as sns



scaler = StandardScaler()
X_encoded_scaled = X_encoded.copy()
X_encoded_scaled[x_num.columns] = scaler.fit_transform(X_encoded[x_num.columns])
X_test_encoded_scaled = X_test_encoded.copy()
X_test_encoded_scaled[x_num.columns] = scaler.transform(X_test_encoded[x_num.columns])

models = {
    "Logistic Regression": LogisticRegression(class_weight='balanced',max_iter=500),
    "SVM": SVC(class_weight='balanced',probability=True),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Random Forest":  RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=10,
                           class_weight='balanced', random_state=42, n_jobs=-1)
}
results = {}

for name, model in models.items():
    model.fit(X_encoded_scaled, y_train)
    preds = model.predict(X_test_encoded_scaled)
    proba = model.predict_proba(X_test_encoded_scaled)[:,1]

    results[name] = {
        "Accuracy": accuracy_score(y_test, preds),
        "Precision (macro)": precision_score(y_test, preds, average='macro'),
        "Recall (macro)": recall_score(y_test, preds, average='macro'),
        "F1 (macro)": f1_score(y_test, preds, average='macro'),
        "Confusion Matrix": confusion_matrix(y_test, preds),
        "AUC": roc_auc_score(y_test, proba)
    }

print(results) #focus is on recall which is catching all the positives(customer who will churn) and good f1 score(balances Precision & Recall)

# Single Tree:        High variance, overfits noise
# Random Forest:      Low variance, smooths errors via averaging

x_test_copy = x_test.copy()
# Use original x_test for group-level analysis, since preprocessed features aren't human-readable.
# Probabilities from the model can be safely mapped back to x_test.

x_test_copy['Predicted_Churn_Prob'] = proba

print(x_test_copy.groupby('SeniorCitizen')['Predicted_Churn_Prob'].mean())#seniorCitizen more likely to churn
print(x_test_copy.groupby('Contract')['Predicted_Churn_Prob'].mean())# Month-to-month  are more likely to churn
print(x_test_copy.groupby('InternetService')['Predicted_Churn_Prob'].mean())#those using Fiber optic are more likely to churn
print(x_test_copy.groupby('PaymentMethod')['Predicted_Churn_Prob'].mean()) # Electronic check  are more likely to churn

"""Hyperparameter tuning"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators' : [10,50],
    'max_depth' : [2,5,7,10],
    'min_samples_split' : [5],
    'class_weight' : ['balanced']
}
grid = GridSearchCV(
    RandomForestClassifier(),
    param_grid= param_grid,
    scoring ='roc_auc',
    n_jobs=-1,
    cv =10
)

grid.fit(X_encoded_scaled, y_train)
best_model= grid.best_estimator_

print("Best Params:", grid.best_params_)
print("Best CV AUC:", grid.best_score_)

from sklearn.metrics import RocCurveDisplay

RocCurveDisplay.from_estimator(best_model, X_test_encoded_scaled, y_test)
plt.title("ROC Curve – Titanic")
plt.show()

from sklearn.metrics import PrecisionRecallDisplay

PrecisionRecallDisplay.from_estimator(best_model, X_test_encoded_scaled, y_test)
plt.title("Precision-Recall Curve – customer churn")
plt.show()

|from sklearn.model_selection import learning_curve
from sklearn.model_selection import LearningCurveDisplay

train_sizes, train_scores, val_scores = learning_curve(
    best_model, X_encoded_scaled, y_train, cv=5, scoring='roc_auc', n_jobs=-1
)

display = LearningCurveDisplay(
    train_sizes=train_sizes,
    train_scores=train_scores,
    test_scores=val_scores
).plot()
# display.plot()
plt.title("Learning Curve – Customer Churn")
plt.show()

